A million bears walking on the streets of Hong Kong. A strawberry frog. A cat made out of spaghetti and meatballs.These are just a few of the text descriptions that people have fed to cutting-edge artificial intelligence systems in recent weeks, which these systems — notably OpenAI's DALL-E 2 and Google Research's Imagen — can use to produce incredibly detailed, realistic-looking images.The resulting pictures can be silly, strange, or even reminiscent of classic art, and they're being shared widely (and sometimes breathlessly) on social media, including by influential figures in the tech community. DALL-E 2 (which is a newer version of a similar, less capable AI system OpenAI rolled out last year) can also edit existing images by adding or taking out objectsIt's not hard to imagine such on-demand image generation eventually serving as a powerful tool for making all kinds of creative content, whether it be art or ads; DALL-E 2 and a similar such system, Midjourney, have already been used to help create magazine covers. OpenAI and Google have pointed to a few ways the technology might be commercialized, such as for editing images or creating stock images.Neither DALL-E 2 nor Imagen is currently available to the public. Yet they share an issue with many others that already are. they can also produce disturbing results that reflect the gender and cultural biases of the data on which they were trained — data that includes millions of images pulled from the internet.The bias in these AI systems presents a serious issue.